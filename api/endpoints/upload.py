from fastapi import APIRouter, UploadFile, File, HTTPException, Depends, Form
from pydantic import BaseModel
from typing import Optional, Dict, Any, List
import time
import logging
import uuid
import base64

from core.config import settings, ALLOWED_CONTENT_TYPES
from services.model_service import model_service
from utils.image_utils import base64_to_image, validate_image_size, format_file_size, get_image_info

# 1. å¯¼å…¥æ‰€éœ€çš„æ•°æ®åº“æ¨¡å‹
from models.image import Image
from models.prediction import Prediction
from .predict import ErrorResponse

logger = logging.getLogger(__name__)
router = APIRouter()


# === ä¿®å¤ 1: æ›´æ–°å“åº”æ¨¡å‹ï¼Œæ·»åŠ ç»“æœå­—æ®µ ===
class FileUploadResponse(BaseModel):
    """æ–‡ä»¶ä¸Šä¼ å“åº”æ¨¡å‹"""
    status: str
    request_id: str
    message: str
    filename: str
    file_size: str
    detected_format: str
    image_info: Optional[Dict[str, Any]] = None
    processing_time: Optional[float] = None
    # ğŸ‘‡ æ–°å¢ï¼šå¿…é¡»æŠŠè¿™äº›å­—æ®µåŠ å›æ¥ï¼Œå‰ç«¯æ‰èƒ½æ”¶åˆ°æ•°æ®ï¼
    result_image: Optional[str] = None
    confidence: Optional[float] = None
    vessel_coverage: Optional[float] = None


# === ä¿®å¤ 2: æ›´æ–°æ¨¡å‹ä¿¡æ¯å“åº”ï¼Œå…¼å®¹çœŸå®æ¨¡å‹ ===
class ModelInfoResponse(BaseModel):
    """æ¨¡å‹ä¿¡æ¯å“åº”æ¨¡å‹"""
    model_name: str
    version: Optional[str] = None  # æ”¹ä¸ºå¯é€‰ï¼Œå…¼å®¹ä¸åŒå‘½å
    status: str
    input_size: Optional[str] = "512x512"
    device: Optional[str] = "CPU/GPU"  # æ–°å¢å­—æ®µ
    # æŠŠæ—§å­—æ®µè®¾ä¸ºå¯é€‰ï¼Œé˜²æ­¢æŠ¥é”™
    output_type: Optional[str] = None
    description: Optional[str] = None
    supported_formats: Optional[List[str]] = None
    performance: Optional[Dict[str, str]] = None
    integration_status: Optional[str] = None


@router.post("/upload/predict",
             response_model=FileUploadResponse,
             summary="æ–‡ä»¶ä¸Šä¼ é¢„æµ‹",
             description="é€šè¿‡æ–‡ä»¶ä¸Šä¼ æ–¹å¼è¿›è¡Œè§†ç½‘è†œè¡€ç®¡åˆ†å‰²é¢„æµ‹",
             responses={
                 500: {"model": ErrorResponse},
                 400: {"model": ErrorResponse},
             },
             )
async def predict_from_upload(
        file: UploadFile = File(...),
        user_id: Optional[str] = Form(None)
):
    """
    æ–‡ä»¶æµæ–¹å¼ä¸Šä¼ å›¾åƒå¹¶è¿›è¡Œé¢„æµ‹
    """
    start_time = time.time()
    request_id = f"file_{int(time.time())}_{uuid.uuid4().hex[:8]}"

    logger.info(f"ğŸ“¤ æ–‡ä»¶ä¸Šä¼ è¯·æ±‚ {request_id} - æ–‡ä»¶å: {file.filename}")

    try:
        # --- éªŒè¯é˜¶æ®µ ---
        if file.content_type not in settings.ALLOWED_IMAGE_TYPES:
            raise HTTPException(status_code=400, detail={"status": "error", "message": "Unsupported file type"})

        contents = await file.read()
        file_size = len(contents)

        if file_size > settings.MAX_FILE_SIZE or file_size == 0:
            raise HTTPException(status_code=400, detail={"status": "error", "message": "File size invalid"})

        detected_format = ALLOWED_CONTENT_TYPES.get(file.content_type, "unknown")

        # è½¬æ¢å›¾åƒ
        image_base64 = base64.b64encode(contents).decode('utf-8')
        image = base64_to_image(image_base64)

        if image is None:
            raise HTTPException(status_code=400, detail={"status": "error", "message": "Invalid image data"})

        is_valid, error_msg = validate_image_size(
            image,
            min_size=(100, 100),
            max_size=(settings.MAX_IMAGE_DIMENSION, settings.MAX_IMAGE_DIMENSION)
        )
        if not is_valid:
            raise HTTPException(status_code=400, detail={"status": "error", "message": error_msg})

        image_info = get_image_info(image)

        # --- é¢„æµ‹é˜¶æ®µ ---
        prediction_result = await model_service.predict(image, request_id)
        processing_time = time.time() - start_time
        formatted_size = format_file_size(file_size)

        # --- æ•°æ®åº“é›†æˆé˜¶æ®µ ---
        if prediction_result["status"] == "success":
            try:
                # ä¿å­˜å›¾ç‰‡
                img_record = Image(
                    user_id=user_id or "anonymous",
                    filename=file.filename,
                    file_size=file_size,
                    content_type=file.content_type
                )
                image_db_id = await img_record.save()

                # ä¿å­˜é¢„æµ‹
                pred_record = Prediction(
                    request_id=request_id,
                    model_version=getattr(model_service, "model_version", "unknown"),
                    result_data={
                        "confidence": prediction_result.get("confidence"),
                        "vessel_coverage": prediction_result.get("vessel_coverage"),
                        "processing_time": processing_time,
                        "image_db_id": image_db_id
                    },
                    user_id=user_id or "anonymous"
                )
                await pred_record.save()
                logger.info(f"ğŸ’¾ [DB] å·²ä¿å­˜è®°å½• (ID: {image_db_id})")
            except Exception as db_e:
                logger.error(f"âš ï¸ [DB] ä¿å­˜å¤±è´¥: {db_e}")

        # --- è¿”å›ç»“æœ (ä¿®å¤é‡ç‚¹) ---
        # å¦‚æœé¢„æµ‹å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
        if prediction_result["status"] != "success":
            raise HTTPException(status_code=500, detail=prediction_result)

        logger.info(f"âœ… é¢„æµ‹æˆåŠŸ {request_id}")

        # ğŸ‘‡ è¿™é‡ŒæŠŠ result_image ç­‰å­—æ®µå¡«è¿›å»ï¼Œå‰ç«¯å°±èƒ½æ”¶åˆ°äº†ï¼
        return FileUploadResponse(
            status="success",
            request_id=request_id,
            message=f"æ–‡ä»¶ '{file.filename}' å¤„ç†æˆåŠŸ",
            filename=file.filename,
            file_size=formatted_size,
            detected_format=detected_format,
            image_info=image_info,
            processing_time=processing_time,
            # æ–°å¢å­—æ®µèµ‹å€¼ï¼š
            result_image=prediction_result.get("result_image"),
            confidence=prediction_result.get("confidence"),
            vessel_coverage=prediction_result.get("vessel_coverage")
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ğŸ’¥ å¼‚å¸¸: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail={
                "status": "error",
                "request_id": request_id,
                "error_code": "INTERNAL_ERROR",
                "message": str(e),
                "timestamp": time.strftime('%Y-%m-%d %H:%M:%S')
            }
        )


@router.get("/model/info", response_model=ModelInfoResponse)
async def get_model_info():
    """è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯æ¥å£ (å…¼å®¹çœŸå®æ¨¡å‹)"""
    # è·å–æœåŠ¡é‡Œçš„åŸå§‹ä¿¡æ¯
    raw_info = model_service.get_model_info()

    # æ‰‹åŠ¨æ˜ å°„ï¼Œé˜²æ­¢ KeyError
    return ModelInfoResponse(
        model_name=raw_info.get("model_name", "Unknown Model"),
        version=raw_info.get("version") or raw_info.get("model_version"),
        status=raw_info.get("status", "unknown"),
        input_size=raw_info.get("input_size", "N/A"),
        device=raw_info.get("device", "CPU"),
        # å…¶ä»–å­—æ®µç»™é»˜è®¤å€¼
        description=raw_info.get("description", "PyTorch Inference Model"),
        supported_formats=["PNG", "JPG", "BMP", "GIF", "TIFF","TIF"],
        integration_status="Ready"
    )


@router.get("/model/stats")
async def get_model_stats():
    """è·å–æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯"""
    stats = model_service.get_service_stats()
    return {
        "status": "success",
        "model_stats": stats,
        "timestamp": time.strftime('%Y-%m-%d %H:%M:%S')
    }