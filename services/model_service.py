import logging
import time
import numpy as np
from typing import Dict, Any
import os
import sys
import torch
import cv2
from datetime import datetime

from core.config import settings
from utils.image_utils import image_to_base64

# === å…³é”®è®¾ç½® ===
# 1. æŠŠ ai_core åŠ å…¥ç³»ç»Ÿè·¯å¾„ï¼Œè¿™æ ·æ‰èƒ½å¯¼å…¥ Unet.py
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'ai_core'))

# 2. å°è¯•å¯¼å…¥æ¨¡åž‹æž¶æž„ (é˜²æ­¢ç¼–è¾‘å™¨æŠ¥é”™)
try:
    # âš ï¸ è¿™é‡Œçš„ç±»åå¿…é¡»å’Œä½  ai_core/Unet.py é‡Œçš„ç±»åä¸€è‡´
    # å¦‚æžœä½ çš„ç±»åæ˜¯ U_Netï¼Œè¯·æ”¹ä¸º: from ai_core.Unet import U_Net as UNet
    from ai_core.Unet import UNet
except ImportError as e:
    print(f"âŒ å¯¼å…¥æ¨¡åž‹æž¶æž„å¤±è´¥: {e}")
    UNet = None

logger = logging.getLogger(__name__)


class ModelService:
    """
    æ¨¡åž‹æœåŠ¡ç±» - æ­£å¼ç‰ˆ
    é›†æˆçœŸå®žçš„ PyTorch U-Net æ¨¡åž‹è¿›è¡ŒæŽ¨ç†
    """

    def __init__(self):
        self.model = None
        self.model_loaded = False
        # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡ï¼šæœ‰æ˜¾å¡ç”¨æ˜¾å¡ï¼Œæ²¡æ˜¾å¡ç”¨CPU
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_name = "U-Net (PyTorch)"
        self.model_version = "1.0.0-release"
        self.load_time = None
        self.prediction_count = 0

        logger.info(f"ðŸŽ¯ æ¨¡åž‹æœåŠ¡åˆå§‹åŒ– (è®¾å¤‡: {self.device})")

    async def load_model(self, model_path: str) -> bool:
        """
        åŠ è½½çœŸå®žæ¨¡åž‹æƒé‡
        """
        try:
            logger.info(f"ðŸ”§ å¼€å§‹åŠ è½½æ¨¡åž‹")
            start_time = time.time()

            # 1. ä¿®æ­£è·¯å¾„ï¼šæŒ‡å‘ ai_core ä¸‹çš„ bestmodel.pt
            real_model_path = os.path.join(os.path.dirname(__file__), '..', 'ai_core', 'bestmodel.pt')

            if not os.path.exists(real_model_path):
                logger.error(f"âŒ æ‰¾ä¸åˆ°æ¨¡åž‹æ–‡ä»¶: {real_model_path}")
                return False

            # 2. åŠ è½½å®Œæ•´æ¨¡åž‹
            # ä½¿ç”¨ torch.load ç›´æŽ¥åŠ è½½æ•´ä¸ªæ¨¡åž‹å¯¹è±¡
            self.model = torch.load(real_model_path, map_location=self.device)

            # 3. è½¬ç§»åˆ°è®¾å¤‡ (CPU æˆ– CUDA)
            self.model.to(self.device)
            self.model.eval()  # å¼€å¯è¯„ä¼°æ¨¡å¼

            self.model_loaded = True
            self.load_time = datetime.now()
            load_duration = time.time() - start_time

            logger.info(f"âœ… æ¨¡åž‹åŠ è½½æˆåŠŸ! è€—æ—¶: {load_duration:.2f}s")
            return True

        except Exception as e:
            logger.error(f"âŒ æ¨¡åž‹åŠ è½½å¤±è´¥: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            self.model_loaded = False
            return False

    async def predict(self, image: np.ndarray, request_id: str) -> Dict[str, Any]:
        """
        ä½¿ç”¨çœŸå®žæ¨¡åž‹è¿›è¡ŒæŽ¨ç†
        """
        if not self.model_loaded:
            return {"status": "error", "message": "æ¨¡åž‹æœªåŠ è½½", "request_id": request_id}

        try:
            start_time = time.time()
            self.prediction_count += 1

            # === 1. å›¾åƒé¢„å¤„ç† ===
            # åŽŸå§‹ image æ˜¯ (H, W, 3) çš„ BGR æ ¼å¼ (OpenCVè¯»å–)
            original_h, original_w = image.shape[:2]

            # è½¬ä¸º RGB
            img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            # è°ƒæ•´å¤§å°åˆ°æ¨¡åž‹è¾“å…¥å°ºå¯¸ (ä¾‹å¦‚ 512x512)
            # âš ï¸ æ³¨æ„ï¼šå¦‚æžœæ¨¡åž‹è®­ç»ƒæ—¶ç”¨çš„ä¸æ˜¯ 512x512ï¼Œè¿™é‡Œéœ€è¦æ”¹
            input_size = (512, 512)
            img_resized = cv2.resize(img_rgb, input_size)

            # å½’ä¸€åŒ– (0-255 -> 0.0-1.0)
            img_normalized = img_resized.astype(np.float32) / 255.0

            # è½¬æ¢ç»´åº¦: (H, W, C) -> (C, H, W)
            img_transposed = img_normalized.transpose((2, 0, 1))

            # è½¬ä¸º Tensor å¹¶å¢žåŠ  Batch ç»´åº¦: (1, C, H, W)
            img_tensor = torch.from_numpy(img_transposed).unsqueeze(0)
            img_tensor = img_tensor.to(self.device)

            # === 2. æ¨¡åž‹æŽ¨ç† ===
            with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦ï¼ŒèŠ‚çœå†…å­˜
                output = self.model(img_tensor)

                # U-Net è¾“å‡ºé€šå¸¸æ˜¯ Logitsï¼Œéœ€è¦ç»è¿‡ Sigmoid å˜æˆæ¦‚çŽ‡ (0-1)
                probs = torch.sigmoid(output)

                # ç§»é™¤ Batch å’Œ Channel ç»´åº¦ -> (H, W)
                probs = probs.squeeze().cpu().numpy()

            # === 3. åŽå¤„ç† ===
            # é˜ˆå€¼å¤„ç†ï¼šå¤§äºŽ 0.5 ç®—è¡€ç®¡ï¼Œå°äºŽ 0.5 ç®—èƒŒæ™¯
            mask = (probs > 0.5).astype(np.uint8) * 255

            # è°ƒæ•´å›žåŽŸå§‹å°ºå¯¸
            if mask.shape != (original_h, original_w):
                mask = cv2.resize(mask, (original_w, original_h), interpolation=cv2.INTER_NEAREST)

            # è®¡ç®—ç½®ä¿¡åº¦å’Œè¦†ç›–çŽ‡ (ç®€å•çš„ç»Ÿè®¡)
            confidence = float(probs.mean())
            vessel_coverage = float(np.count_nonzero(mask) / mask.size)

            # è½¬ä¸º Base64
            result_base64 = image_to_base64(mask, "png")
            actual_time = time.time() - start_time

            logger.info(f"âœ… çœŸå®žé¢„æµ‹å®Œæˆ [{request_id}]")

            return {
                "status": "success",
                "request_id": request_id,
                "result_image": result_base64,
                "processing_time": actual_time,
                "confidence": round(confidence, 4),
                "vessel_coverage": round(vessel_coverage, 4),
                "message": "é¢„æµ‹æˆåŠŸ (çœŸå®žæ¨¡åž‹)"
            }

        except Exception as e:
            logger.error(f"âŒ é¢„æµ‹è¿‡ç¨‹å‡ºé”™: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                "status": "error",
                "request_id": request_id,
                "message": f"é¢„æµ‹å¤±è´¥: {str(e)}"
            }

    def get_model_info(self) -> Dict[str, Any]:
        return {
            "model_name": self.model_name,
            "version": self.model_version,
            "status": "loaded" if self.model_loaded else "error",
            "device": str(self.device),
            "input_size": "512x512"
        }

    def get_service_stats(self) -> Dict[str, Any]:
        return {
            "total_predictions": self.prediction_count,
            "uptime": str(datetime.now() - self.load_time) if self.load_time else "N/A"
        }


# åˆ›å»ºå…¨å±€å®žä¾‹
model_service = ModelService()