import logging
import time
import numpy as np
from typing import Dict, Any, Optional
import uuid
from datetime import datetime
import os
import random
import cv2
from core.config import settings
from utils.image_utils import create_mock_segmentation, image_to_base64
from PIL import Image

logger = logging.getLogger(__name__)

# å®šä¹‰ mock æ•°æ®ç›®å½•çš„è·¯å¾„
MOCK_DATA_DIR = os.path.join(os.path.dirname(__file__), '..', 'mock_data')
MOCK_MASKS = []

# å¯åŠ¨æ—¶å°±åŠ è½½æ‰€æœ‰å¯ç”¨çš„ mock æ©ç è·¯å¾„
if os.path.exists(MOCK_DATA_DIR):
    try:
        MOCK_MASKS = [
            os.path.join(MOCK_DATA_DIR, f)
            for f in os.listdir(MOCK_DATA_DIR)
            if f.endswith(('.png', '.jpg', '.tiff','.gif'))
        ]
        if MOCK_MASKS:
            logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(MOCK_MASKS)} ä¸ªæ¨¡æ‹Ÿæ©ç æ–‡ä»¶")
        else:
            logger.warning("âš ï¸ mock_data ç›®å½•ä¸ºç©ºï¼Œæ¨¡æ‹Ÿé¢„æµ‹å°†å¤±è´¥")
    except Exception as e:
        logger.error(f"âŒ åŠ è½½æ¨¡æ‹Ÿæ©ç å¤±è´¥: {e}")

class ModelService:
    """æ¨¡å‹æœåŠ¡ç±» - ç®¡ç†AIæ¨¡å‹çš„åŠ è½½å’Œé¢„æµ‹ï¼ˆå½“å‰ä¸ºæ¨¡æ‹Ÿç‰ˆæœ¬ï¼‰"""

    def __init__(self):
        self.model = None
        self.model_loaded = False
        self.model_name = "U-Netè§†ç½‘è†œè¡€ç®¡åˆ†å‰²æ¨¡å‹"
        self.model_version = "2.0.0-dev"
        self.load_time = None
        self.prediction_count = 0

        logger.info("ğŸ¯ æ¨¡å‹æœåŠ¡åˆå§‹åŒ–å®Œæˆ")

    async def load_model(self, model_path: str) -> bool:
        """
        åŠ è½½AIæ¨¡å‹ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰

        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„

        Returns:
            åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info(f"ğŸ”§ å¼€å§‹åŠ è½½æ¨¡å‹: {model_path}")

            # æ¨¡æ‹Ÿæ¨¡å‹åŠ è½½è¿‡ç¨‹
            start_time = time.time()
            await self._simulate_model_loading()

            # è®¾ç½®æ¨¡å‹çŠ¶æ€
            self.model_loaded = True
            self.load_time = datetime.now()

            load_duration = time.time() - start_time

            logger.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ - è€—æ—¶: {load_duration:.2f}ç§’")
            logger.info(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯: {self.model_name} v{self.model_version}")

            return True

        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            self.model_loaded = False
            return False

    async def _simulate_model_loading(self):
        """æ¨¡æ‹Ÿæ¨¡å‹åŠ è½½è¿‡ç¨‹"""
        logger.info("â³ æ¨¡æ‹Ÿæ¨¡å‹åŠ è½½ä¸­...")

        # æ¨¡æ‹Ÿä¸åŒçš„åŠ è½½æ­¥éª¤
        steps = [
            "åˆå§‹åŒ–æ¨¡å‹æ¶æ„",
            "åŠ è½½é¢„è®­ç»ƒæƒé‡",
            "é…ç½®è®¡ç®—è®¾å¤‡",
            "ä¼˜åŒ–æ¨ç†è®¾ç½®",
            "éªŒè¯æ¨¡å‹å®Œæ•´æ€§"
        ]

        for i, step in enumerate(steps, 1):
            logger.info(f"  [{i}/{len(steps)}] {step}")
            time.sleep(0.5)  # æ¯ä¸ªæ­¥éª¤0.5ç§’

    async def predict(self, image: np.ndarray, request_id: str) -> Dict[str, Any]:
        """
        è¿›è¡Œè¡€ç®¡åˆ†å‰²é¢„æµ‹ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰
        *** å·²å‡çº§ï¼šè¿”å› mock_data ä¸­çš„çœŸå®æ©ç  ***
        """
        if not self.model_loaded:
            return {
                "status": "error",
                "message": "æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•è¿›è¡Œé¢„æµ‹",
                "request_id": request_id
            }

        try:
            start_time = time.time()
            self.prediction_count += 1

            logger.info(f"ğŸ” å¼€å§‹è¡€ç®¡åˆ†å‰²é¢„æµ‹ [{request_id}] (æ¨¡æ‹Ÿ)")
            logger.info(f"ğŸ“ è¾“å…¥å›¾åƒå°ºå¯¸: {image.shape}")

            # æ¨¡æ‹Ÿå¤„ç†è¿‡ç¨‹
            await self._simulate_prediction_processing()

            if not MOCK_MASKS:
                logger.error(f"âŒ æ¨¡æ‹Ÿé¢„æµ‹å¤±è´¥: mock_data ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°æ©ç æ–‡ä»¶")
                raise Exception("æ¨¡æ‹Ÿæ•°æ®æœªæ‰¾åˆ°")

                # 1. éšæœºé€‰æ‹©ä¸€ä¸ªæ©ç 
            random_mask_path = random.choice(MOCK_MASKS)
            logger.info(f"ğŸ¨ æ­£åœ¨ä½¿ç”¨æ¨¡æ‹Ÿæ©ç : {os.path.basename(random_mask_path)}")

            # 2. ã€ä½¿ç”¨ PIL (Pillow) è¯»å– GIF æ–‡ä»¶ã€‘
            pil_image = Image.open(random_mask_path)

            # 3. ã€è½¬æ¢ä¸ºç°åº¦å›¾å¹¶è½¬ä¸º NumPy æ•°ç»„ã€‘
            #    .convert('L') ç¡®ä¿å®ƒæ˜¯å•é€šé“ç°åº¦å›¾
            #    np.array(...) å°† PIL å›¾åƒè½¬ä¸º cv2 å¯ä»¥å¤„ç†çš„ NumPy æ•°ç»„
            segmentation_mask = np.array(pil_image.convert('L'))

            if segmentation_mask is None:
                logger.error(f"âŒ æ— æ³•è¯»å–æˆ–è½¬æ¢æ¨¡æ‹Ÿæ©ç æ–‡ä»¶: {random_mask_path}")
                raise Exception("æ¨¡æ‹Ÿæ•°æ®è¯»å–å¤±è´¥")

            # ç¡®ä¿æ©ç å’Œè¾“å…¥å›¾åƒå°ºå¯¸ä¸€è‡´
            segmentation_mask = cv2.resize(segmentation_mask, (image.shape[1], image.shape[0]))


            result_base64 = image_to_base64(segmentation_mask, "png")

            actual_time = time.time() - start_time

            # æ¨¡æ‹Ÿçš„ç½®ä¿¡åº¦å’Œè¦†ç›–ç‡
            image_quality = min(1.0, (image.shape[0] * image.shape[1]) / (1000 * 1000))
            confidence = 0.7 + (image_quality * 0.2) + (np.random.random() * 0.1)
            confidence = min(0.95, confidence)

            logger.info(f"âœ… é¢„æµ‹å®Œæˆ [{request_id}] - è€—æ—¶: {actual_time:.2f}ç§’")
            logger.info(f"ğŸ“Š é¢„æµ‹ç»Ÿè®¡ - ç½®ä¿¡åº¦: {confidence:.2f}, æ€»é¢„æµ‹æ¬¡æ•°: {self.prediction_count}")

            return {
                "status": "success",
                "request_id": request_id,
                "segmentation_mask": segmentation_mask,  # (è¿™ä¸ªä¸»è¦ç”¨äºå†…éƒ¨ï¼Œéè¿”å›)
                "result_image": result_base64,  # (è¿™ä¸ªè¿”å›ç»™å‰ç«¯)
                "processing_time": actual_time,
                "confidence": round(confidence, 3),
                "vessel_coverage": round(
                    np.sum(segmentation_mask > 0) / (segmentation_mask.shape[0] * segmentation_mask.shape[1]), 4),
                "message": "è¡€ç®¡åˆ†å‰²å®Œæˆï¼ˆæ¨¡æ‹Ÿç»“æœï¼‰- å·²å‡çº§ä¸ºçœŸå®æ©ç "
            }

        except Exception as e:
            logger.error(f"âŒ é¢„æµ‹è¿‡ç¨‹å‡ºé”™ [{request_id}]: {str(e)}")
            return {
                "status": "error",
                "request_id": request_id,
                "message": f"é¢„æµ‹å¤±è´¥: {str(e)}"
            }

    async def _simulate_prediction_processing(self):
        """æ¨¡æ‹Ÿé¢„æµ‹å¤„ç†è¿‡ç¨‹"""
        # æ¨¡æ‹Ÿç¥ç»ç½‘ç»œæ¨ç†æ­¥éª¤
        steps = [
            "å›¾åƒé¢„å¤„ç†",
            "ç‰¹å¾æå–",
            "ç¼–ç å™¨å¤„ç†",
            "è§£ç å™¨å¤„ç†",
            "åå¤„ç†ä¼˜åŒ–",
            "ç»“æœç”Ÿæˆ"
        ]

        for step in steps:
            time.sleep(0.1)  # æ¯ä¸ªæ­¥éª¤0.1ç§’

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯"""
        return {
            "model_name": self.model_name,
            "model_version": self.model_version,
            "status": "loaded" if self.model_loaded else "not_loaded",
            "load_time": self.load_time.isoformat() if self.load_time else None,
            "prediction_count": self.prediction_count,
            "input_size": "512x512 RGBå›¾åƒ",
            "output_type": "äºŒå€¼åˆ†å‰²æ©ç ",
            "supported_formats": ["PNG", "JPEG", "TIFF"],
            "description": "U-Netæ¶æ„çš„è§†ç½‘è†œè¡€ç®¡åˆ†å‰²æ¨¡å‹ - å½“å‰ä¸ºæ¨¡æ‹Ÿç‰ˆæœ¬",
            "performance": {
                "estimated_accuracy": "95%+ (æ¨¡æ‹Ÿ)",
                "processing_time": "1-3ç§’ (æ¨¡æ‹Ÿ)",
                "memory_usage": "~2GB (ä¼°ç®—)"
            },
            "integration_status": "ç­‰å¾…AIç»„äº¤ä»˜çœŸå®æ¨¡å‹"
        }

    def get_service_stats(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "model_loaded": self.model_loaded,
            "total_predictions": self.prediction_count,
            "uptime": str(datetime.now() - self.load_time) if self.load_time else "æœªåŠ è½½",
            "service_status": "æ­£å¸¸è¿è¡Œ" if self.model_loaded else "ç­‰å¾…æ¨¡å‹åŠ è½½"
        }


# åˆ›å»ºå…¨å±€æ¨¡å‹æœåŠ¡å®ä¾‹
model_service = ModelService()