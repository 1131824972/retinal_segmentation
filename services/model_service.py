import logging
import time
import numpy as np
from typing import Dict, Any
import os
import sys
import torch
import cv2
from datetime import datetime

from core.config import settings
from utils.image_utils import image_to_base64

# === å…³é”®è®¾ç½® ===
# 1. æŠŠ ai_core åŠ å…¥ç³»ç»Ÿè·¯å¾„
AI_CORE_PATH = os.path.join(os.path.dirname(__file__), '..', 'ai_core')
sys.path.append(AI_CORE_PATH)

logger = logging.getLogger(__name__)


class ModelService:
    """
    æ¨¡å‹æœåŠ¡ç±» - æ­£å¼ç‰ˆ
    é›†æˆçœŸå®çš„ PyTorch U-Net æ¨¡å‹è¿›è¡Œæ¨ç†
    """

    def __init__(self):
        self.model = None
        self.model_loaded = False
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_name = "U-Net (PyTorch)"
        self.model_version = "1.0.0-release"
        self.load_time = None
        self.prediction_count = 0

        logger.info(f"ğŸ¯ æ¨¡å‹æœåŠ¡åˆå§‹åŒ– (è®¾å¤‡: {self.device})")

    async def load_model(self, model_path: str) -> bool:
        """åŠ è½½çœŸå®æ¨¡å‹æƒé‡"""
        try:
            logger.info(f"ğŸ”§ å¼€å§‹åŠ è½½æ¨¡å‹...")
            start_time = time.time()

            real_model_path = os.path.join(AI_CORE_PATH, 'bestmodel.pt')

            if not os.path.exists(real_model_path):
                logger.error(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {real_model_path}")
                return False

            # 2. åŠ è½½å®Œæ•´æ¨¡å‹
            # weights_only=False è§£å†³ FutureWarning
            self.model = torch.load(real_model_path, map_location=self.device, weights_only=False)

            self.model.to(self.device)
            self.model.eval()

            self.model_loaded = True
            self.load_time = datetime.now()
            load_duration = time.time() - start_time

            logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ! è€—æ—¶: {load_duration:.2f}s")
            return True

        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            self.model_loaded = False
            return False

    async def predict(self, image: np.ndarray, request_id: str) -> Dict[str, Any]:
        """ä½¿ç”¨çœŸå®æ¨¡å‹è¿›è¡Œæ¨ç† (Debug ç‰ˆ)"""
        if not self.model_loaded:
            return {"status": "error", "message": "æ¨¡å‹æœªåŠ è½½", "request_id": request_id}

        try:
            start_time = time.time()
            self.prediction_count += 1

            # === 1. å›¾åƒé¢„å¤„ç† ===
            original_h, original_w = image.shape[:2]
            img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            img_resized = cv2.resize(img_rgb, (512, 512))

            # å½’ä¸€åŒ– (åŒ¹é… dataset.py)
            img_float = img_resized.astype(np.float32)
            min_val = np.min(img_float)
            max_val = np.max(img_float)
            if max_val - min_val > 1e-5:
                img_normalized = (img_float - min_val) / (max_val - min_val)
            else:
                img_normalized = img_float / 255.0

            img_transposed = img_normalized.transpose((2, 0, 1))
            img_tensor = torch.from_numpy(img_transposed).unsqueeze(0).to(self.device)

            # === 2. æ¨¡å‹æ¨ç† ===
            with torch.no_grad():
                output = self.model(img_tensor)

                # ğŸ›‘ã€å…³é”® Debugã€‘æ‰“å°æ¨¡å‹è¾“å‡ºå½¢çŠ¶
                logger.info(f"ğŸ” [Debug] æ¨¡å‹åŸå§‹è¾“å‡º Shape: {output.shape}")

                # å¤„ç†è¾“å‡º
                # å¦‚æœæ˜¯å¤šåˆ†ç±» (Batch, 2, H, W)ï¼Œé€šå¸¸ Channel 1 æ˜¯è¡€ç®¡
                if output.shape[1] == 2:
                    logger.info("ğŸ” [Debug] æ£€æµ‹åˆ°åŒé€šé“è¾“å‡ºï¼Œå–ç¬¬2ä¸ªé€šé“ (Index 1) ä½œä¸ºè¡€ç®¡")
                    # å–å‡ºè¡€ç®¡é€šé“ï¼Œå¹¶ä¿ç•™ç»´åº¦ä»¥ä¾¿åç»­å¤„ç†
                    output_vessel = output[:, 1, :, :].unsqueeze(1)
                else:
                    # å•é€šé“ç›´æ¥ç”¨
                    output_vessel = output

                # âš ï¸ å°è¯•ç§»é™¤ Sigmoidï¼Œç›´æ¥çœ‹åŸå§‹å€¼åˆ†å¸ƒ
                # å¾ˆå¤š U-Net æœ€åä¸€å±‚å·²ç»æ˜¯ Sigmoid äº†ï¼Œæˆ–è€…è¾“å‡ºå°±æ˜¯æ¦‚ç‡
                probs = output_vessel.squeeze().cpu().numpy()

                # ğŸ›‘ã€å…³é”® Debugã€‘æ‰“å°æ•°å€¼èŒƒå›´ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦ Sigmoid
                logger.info(f"ğŸ” [Debug] è¾“å‡ºæ•°å€¼èŒƒå›´: Min={probs.min():.4f}, Max={probs.max():.4f}")

                # åŠ¨æ€å†³ç­–ï¼šå¦‚æœæ•°å€¼åœ¨ [0, 1] ä¹‹å¤–ï¼ˆæ¯”å¦‚ -10, +10ï¼‰ï¼Œè¯´æ˜éœ€è¦ Sigmoid
                if probs.min() < 0 or probs.max() > 1.5:
                    logger.info("ğŸ” [Debug] æ•°å€¼è¶…å‡º [0,1]ï¼Œåº”ç”¨ Sigmoid æ¿€æ´»")
                    probs = 1 / (1 + np.exp(-probs))  # NumPy ç‰ˆ Sigmoid

            # === 3. åå¤„ç† ===
            # é˜ˆå€¼åŒ–
            mask = (probs > 0.5).astype(np.uint8) * 255

            if mask.shape != (original_h, original_w):
                mask = cv2.resize(mask, (original_w, original_h), interpolation=cv2.INTER_NEAREST)

            confidence = float(probs.mean())
            vessel_coverage = float(np.count_nonzero(mask) / mask.size)

            result_base64 = image_to_base64(mask, "png")
            actual_time = time.time() - start_time

            logger.info(f"âœ… çœŸå®é¢„æµ‹å®Œæˆ [{request_id}]")

            return {
                "status": "success",
                "request_id": request_id,
                "result_image": result_base64,
                "processing_time": actual_time,
                "confidence": round(confidence, 4),
                "vessel_coverage": round(vessel_coverage, 4),
                "message": "é¢„æµ‹æˆåŠŸ"
            }

        except Exception as e:
            logger.error(f"âŒ é¢„æµ‹å¼‚å¸¸: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return {"status": "error", "request_id": request_id, "message": str(e)}

    def get_model_info(self) -> Dict[str, Any]:
        return {
            "model_name": self.model_name,
            "version": self.model_version,
            "status": "loaded" if self.model_loaded else "error",
            "device": str(self.device),
            "input_size": "512x512"
        }

    def get_service_stats(self) -> Dict[str, Any]:
        return {
            "total_predictions": self.prediction_count,
            "uptime": str(datetime.now() - self.load_time) if self.load_time else "N/A"
        }


# åˆ›å»ºå…¨å±€å®ä¾‹
model_service = ModelService()